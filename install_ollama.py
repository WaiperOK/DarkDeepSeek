"""
–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama –∏ DeepSeek-R1-8B
–ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ my-pentest-gpt —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
"""
import os
import sys
import subprocess
import platform
import requests
from pathlib import Path

def print_step(message: str):
    """–ü–µ—á–∞—Ç–∞–µ—Ç —à–∞–≥ —É—Å—Ç–∞–Ω–æ–≤–∫–∏"""
    print(f"\nüîß {message}")
    print("=" * 50)

def run_command(command: str, check: bool = True) -> bool:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–∞–Ω–¥—É –≤ shell"""
    try:
        result = subprocess.run(command, shell=True, check=check, capture_output=True, text=True)
        if result.stdout:
            print(result.stdout)
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã: {e}")
        if e.stderr:
            print(f"Stderr: {e.stderr}")
        return False

def check_ollama_installed() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª–∏ Ollama"""
    try:
        result = subprocess.run(["ollama", "--version"], capture_output=True, text=True)
        return result.returncode == 0
    except FileNotFoundError:
        return False

def install_ollama():
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç Ollama –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –û–°"""
    system = platform.system().lower()

    if system == "linux":
        print("–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama –¥–ª—è Linux...")
        return run_command("curl -fsSL https://ollama.ai/install.sh | sh")

    elif system == "darwin":
        print("–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama –¥–ª—è macOS...")
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º Ollama.app...")
        if run_command("which brew", check=False):
            return run_command("brew install ollama")
        else:
            print("‚ùå Homebrew –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama –≤—Ä—É—á–Ω—É—é:")
            print("https://ollama.ai/download/mac")
            return False

    elif system == "windows":
        print("–î–ª—è Windows —Å–∫–∞—á–∞–π—Ç–µ Ollama —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞:")
        print("https://ollama.ai/download/windows")
        print("–ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç.")
        return False

    else:
        print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –û–°: {system}")
        return False

def start_ollama_service():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–µ—Ä–≤–∏—Å Ollama"""
    print("–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ Ollama...")

    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama —É–∂–µ –∑–∞–ø—É—â–µ–Ω–∞")
            return True
    except:
        pass

    system = platform.system().lower()

    if system in ["linux", "darwin"]:
        subprocess.Popen(["ollama", "serve"],
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL)

        import time
        for i in range(30):
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=2)
                if response.status_code == 200:
                    print("‚úÖ Ollama —Å–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω")
                    return True
            except:
                pass
            time.sleep(1)
            print(f"–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞... {i+1}/30")

        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å Ollama —Å–µ—Ä–≤–∏—Å")
        return False

    else:
        print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama –≤—Ä—É—á–Ω—É—é: ollama serve")
        return True

def pull_deepseek_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å DeepSeek-R1-8B"""
    print("–ó–∞–≥—Ä—É–∑–∫–∞ DeepSeek-R1-8B –º–æ–¥–µ–ª–∏...")
    print("‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç (–º–æ–¥–µ–ª—å ~5GB)")

    models_to_try = [
        "deepseek-r1:8b-distill-q4_K_M",
        "deepseek-r1:8b",
        "deepseek-r1",
        "deepseek-coder:6.7b-instruct-q4_K_M"
    ]

    for model in models_to_try:
        print(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {model}")
        if run_command(f"ollama pull {model}", check=False):
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            return model
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {model}")

    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å DeepSeek")
    print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—Ä—É—á–Ω—É—é:")
    print("ollama pull deepseek-coder:6.7b-instruct-q4_K_M")
    return None

def test_installation():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫—É"""
    print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏...")

    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code != 200:
            print("‚ùå Ollama API –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
            return False

        models = response.json().get("models", [])
        if not models:
            print("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return False

        print("‚úÖ –ù–∞–π–¥–µ–Ω—ã –º–æ–¥–µ–ª–∏:")
        for model in models:
            print(f"  ‚Ä¢ {model.get('name', 'Unknown')}")

        print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
        test_data = {
            "model": models[0]["name"],
            "prompt": "–ù–∞–ø–∏—à–∏ –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é Hello World –Ω–∞ Python:",
            "options": {"num_predict": 100}
        }

        response = requests.post(
            "http://localhost:11434/api/generate",
            json=test_data,
            timeout=60
        )

        if response.status_code == 200:
            result = response.json()
            generated_text = result.get("response", "")
            if generated_text:
                print("‚úÖ –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ!")
                print("–ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
                print("-" * 30)
                print(generated_text[:200] + "..." if len(generated_text) > 200 else generated_text)
                print("-" * 30)
                return True

        print("‚ùå –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ –ø—Ä–æ—à–µ–ª")
        return False

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False

def setup_my_pentest_gpt():
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç my-pentest-gpt"""
    print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ my-pentest-gpt...")

    if not Path("src").exists():
        print("‚ùå –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç –∏–∑ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞")
        return False

    print("–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
    if not run_command(f"{sys.executable} -m pip install -r requirements.txt"):
        print("‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π")
        return False

    print("–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π...")
    if not run_command(f"{sys.executable} src/config.py"):
        print("‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π")
        return False

    print("‚úÖ my-pentest-gpt –Ω–∞—Å—Ç—Ä–æ–µ–Ω!")
    return True

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏"""
    print("üõ°Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ my-pentest-gpt —Å Ollama + DeepSeek-R1-8B")
    print("=" * 60)

    print_step("–ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama")
    if not check_ollama_installed():
        print("Ollama –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º...")
        if not install_ollama():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama")
            sys.exit(1)
    else:
        print("‚úÖ Ollama —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

    print_step("–ó–∞–ø—É—Å–∫ Ollama —Å–µ—Ä–≤–∏—Å–∞")
    if not start_ollama_service():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å Ollama")
        print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—Ä—É—á–Ω—É—é: ollama serve")
        sys.exit(1)

    print_step("–ó–∞–≥—Ä—É–∑–∫–∞ DeepSeek –º–æ–¥–µ–ª–∏")
    model_name = pull_deepseek_model()
    if not model_name:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
        sys.exit(1)

    print_step("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ my-pentest-gpt")
    if not setup_my_pentest_gpt():
        print("‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞")
        sys.exit(1)

    print_step("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏")
    if not test_installation():
        print("‚ùå –¢–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ—à–ª–∏")
        sys.exit(1)

    print("\n" + "=" * 60)
    print("üéâ –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
    print("=" * 60)

    print(f"\n‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
    print("‚úÖ Ollama —Å–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω")
    print("‚úÖ my-pentest-gpt –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

    print("\nüöÄ –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø:")
    print("-" * 30)
    print("# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL injection —ç–∫—Å–ø–ª–æ–π—Ç–∞:")
    print("python -m src.cli_ollama generate generate_exploit --target 'SQL injection –≤ —Ñ–æ—Ä–º–µ –≤—Ö–æ–¥–∞'")
    print()
    print("# –ê–Ω–∞–ª–∏–∑ —É—è–∑–≤–∏–º–æ—Å—Ç–∏:")
    print("python -m src.cli_ollama generate analyze_vulnerability --target 'XSS –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö'")
    print()
    print("# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç:")
    print("python -m src.cli_ollama chat")
    print()
    print("# –°–ø–∏—Å–æ–∫ —à–∞–±–ª–æ–Ω–æ–≤:")
    print("python -m src.cli_ollama list-templates")
    print()
    print("# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π:")
    print("python -m src.cli_ollama list-models")

    print("\nüìö –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø:")
    print("README.md - –ø–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è")
    print("src/cli_ollama.py - CLI —Å Ollama")

    print("\n‚ö†Ô∏è –≠–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:")
    print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª–µ–π")
    print("–∏ —ç—Ç–∏—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏!")

if __name__ == "__main__":
    main()
